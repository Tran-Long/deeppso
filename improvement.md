# Deep PSO for TSP — Detailed Analysis & Improvement Plan

## 1. Executive Summary

The core idea — using a neural network to predict per-particle PSO hyperparameters (w, c1, c2) trained via REINFORCE — is sound. However, several design choices are working against each other, resulting in a system where the PSO dynamics produce negligible improvement over random initialization. The issues span **particle representation**, **PSO dynamics**, **RL training signal quality**, and **architecture bottlenecks**.

---

## 2. Critical Issues (High Impact)

### 2.1 Near-Identical Particle Initialization Kills PSO Diversity

**File:** `pso/particle.py`, `initialize_population()`

```python
particle = torch.ones(self.dim, ...) + torch.randn(self.dim, ...) * 0.1
```

All 256 particles start as `1.0 + N(0, 0.01)` — essentially identical 400-dimensional vectors. Since `decode_solutions()` uses **greedy argmax**, particles with nearly identical edge logits decode to the **same tour**. This means:

- All particles produce the same (or nearly the same) cost.
- `pbest ≈ population` for every particle after init → the cognitive term `c1 * (pbest - x) ≈ 0`.
- `gbest ≈ population[i]` for all `i` → the social term `c2 * (gbest - x) ≈ 0`.
- **PSO velocity updates are near-zero** regardless of what the network predicts for w, c1, c2.

**Impact:** The neural network's output has almost no effect on the trajectory. The REINFORCE signal is pure noise.

**Fix:**
```python
# Option A: Diverse random initialization
particle = torch.randn(self.dim, dtype=torch.float, device=self.device)

# Option B: Uniform initialization with meaningful spread
particle = torch.rand(self.dim, dtype=torch.float, device=self.device) * 2 - 1
```

### 2.2 Greedy Argmax Decoding Creates Piecewise-Constant Cost Landscape

**File:** `pso/particle.py`, `decode_solutions()`

```python
actions = torch.argmax(masked_mat, dim=1)  # GREEDY
```

Argmax is piecewise constant — many different particle positions map to the **exact same tour**. This means:
- Small PSO velocity updates often produce **zero change** in cost.
- The REINFORCE gradient is zero for most perturbations (no cost difference → no learning signal).
- PSO has no gradient-like information to follow; it's navigating a flat plateau.

The commented-out **stochastic decode** (using `dist.Categorical(logits=...)`) is much better for training because it produces cost variation under small position changes.

**Fix:** Use stochastic decoding during training, greedy during evaluation:
```python
def decode_solutions(self, stochastic=True):
    ...
    if stochastic:
        d = dist.Categorical(logits=masked_mat)
        actions = d.sample()
    else:
        actions = torch.argmax(masked_mat, dim=1)
    ...
```

However, note that the log_probs from this stochastic decoding are **not** being included in the REINFORCE loss — only the wc1c2 log_probs are. This is a design choice: you're treating the decoder as part of the environment. This is valid, but you might get stronger signal if you also include decoder log_probs (see §3.3).

### 2.3 Per-Step Network Update Destabilizes the Trajectory

**File:** `deep_pso_module.py`, `training_step()`

```python
for _iter in range(self.pso_iterations_train):
    wc1c2_mu, wc1c2_sigma = self.net(...)
    ...
    self.manual_backward(loss)
    opt.step()        # <-- Network weights change HERE
    opt.zero_grad()
    ...
    particle_population.update_metadata(costs)
```

The network is updated **inside** each PSO iteration. This means:
- At step `t+1`, the network has different weights from step `t`.
- The trajectory the particles follow is generated by a **non-stationary policy**.
- Particle metadata (pbest, gbest) was computed under the old policy, but the next step uses a new policy.
- This introduces bias and instability into the REINFORCE estimator.

**Fix — Option A (Accumulate, then update):** Accumulate gradients across all PSO iterations, then do a single optimizer step:
```python
opt.zero_grad()
total_loss = 0
for _iter in range(self.pso_iterations_train):
    wc1c2_mu, wc1c2_sigma = self.net(...)
    ...
    total_loss += loss
total_loss.backward()
opt.step()
```

**Fix — Option B (Detach between steps):** If you want per-step updates, at least be explicit that each step is independent:
```python
for _iter in range(self.pso_iterations_train):
    with torch.no_grad():
        # Freeze network output for the PSO step
        wc1c2_mu, wc1c2_sigma = self.net(...)
    # ... but then you can't compute gradients
```

**Recommendation:** Option A is cleaner and more theoretically sound. Treat the entire PSO trajectory (all iterations) as one episode, accumulate the loss, then update once.

### 2.4 No Velocity Clamping — Population Divergence

**File:** `pso/particle.py`, `step()`

```python
self.velocity = w * self.velocity + c1 * (self.pbest - self.population) + c2 * (self.gbest - self.population)
self.population = self.population + self.velocity
```

With `w` in `[0.4, 0.9]`, `c1` in `[1.0, 3.0]`, `c2` in `[1.0, 3.0]`:
- Even with random r1, r2, the sum `w + c1*r1 + c2*r2` can exceed 1.0 easily.
- Without velocity clamping, positions and velocities can grow unboundedly.
- After a few steps, particle values can be in the hundreds or thousands.
- `argmax` still works on these (it's scale-invariant for a single row), but the PSO dynamics become chaotic.

**Fix:**
```python
def step(self, wc1c2, using_random=False):
    ...
    self.velocity = w * self.velocity + c1 * (self.pbest - self.population) + c2 * (self.gbest - self.population)
    # Clamp velocity
    v_max = 4.0  # Hyperparameter
    self.velocity = torch.clamp(self.velocity, -v_max, v_max)
    self.population = self.population + self.velocity
```

Additionally, consider using the **constriction factor** (Clerc & Kennedy, 2002) which analytically ensures convergence:
```python
phi = c1 + c2  # Should be > 4
chi = 2.0 / abs(2 - phi - sqrt(phi**2 - 4*phi))
self.velocity = chi * (self.velocity + ...)  # Constriction
```

---

## 3. Significant Issues (Medium Impact)

### 3.1 Reward Signal: Absolute Cost vs. Improvement

**File:** `deep_pso_module.py`

```python
baseline = costs.mean()
loss = ((costs - baseline) * log_probs.sum(dim=-1)).mean()
```

Using the **absolute tour cost** as the reward has high variance. A particle getting cost 4.2 vs 4.3 on a random TSP-20 instance is a tiny relative difference, but different TSP instances have different optimal costs. The mean baseline helps, but the signal-to-noise ratio is still poor.

**Fix — Use improvement as reward:**
```python
# Track previous best cost for each particle
if _iter == 0:
    prev_costs = costs.clone()
improvement = prev_costs - costs  # Positive = improved
baseline = improvement.mean()
loss = -((improvement - baseline) * log_probs.sum(dim=-1)).mean()
prev_costs = torch.min(prev_costs, costs)
```

Or normalize costs by a problem-specific reference (e.g., nearest-neighbor heuristic cost).

### 3.2 Swarm Encoder Information Bottleneck

**File:** `nets/model.py`, `ParticleVectorStem`

```python
def forward(self, *args):
    x = torch.stack(args, dim=-1)  # (n_particles, dim=400, 3)
    x = self.stem(x)               # (n_particles, dim=400, emb_dim)
    x = x.mean(dim=1)              # (n_particles, emb_dim) ← kills spatial info
```

Mean-pooling over the 400-dimensional particle vector (which corresponds to specific edges in the TSP graph) destroys all structural information. The network cannot distinguish which edges are important.

**Fix — Use edge-aware aggregation:**
```python
# Option A: Reshape to (n_particles, n_cities, k_sparse, 3) and use the graph structure
x = x.view(n_particles, n_cities, k_sparse, -1)
# Pool over k_sparse neighbors per city, then process per-city features

# Option B: Use the TSP edge_index to aggregate particle features
# Scatter particle edge values back onto the graph and use a GNN

# Option C: At minimum, use max-pool alongside mean-pool
x_mean = x.mean(dim=1)
x_max = x.max(dim=1).values
x = torch.cat([x_mean, x_max], dim=-1)
```

### 3.3 TSP Embedding Recomputed Unnecessarily

**File:** `nets/tsp_vector.py`, `HyperparamTSPModel.forward()`

The TSP graph doesn't change across PSO iterations within one training step, but `self.tsp_emb(...)` is called every iteration.

**Fix:** Cache the TSP embedding per problem instance:
```python
# In training_step:
pyg_data = problem.pyg_data.to(self.device)
tsp_embedding = self.net.tsp_emb(pyg_data.x, pyg_data.edge_index, pyg_data.edge_attr)
tsp_embedding = tsp_embedding.mean(dim=0, keepdim=True).detach()

for _iter in range(self.pso_iterations_train):
    wc1c2_mu, wc1c2_sigma = self.net.forward_with_cached_tsp(
        ..., tsp_embedding=tsp_embedding
    )
```

### 3.4 Cross-Attention Context is Too Small

**File:** `nets/tsp_vector.py`

```python
context = torch.cat([tsp_embedding, gbest_embedding], dim=0)  # shape: (2, emb_dim)
```

The query (n_particles tokens) attends over only **2 key-value entries**. Multi-head attention with 4 heads over 2 tokens is essentially just a weighted average of tsp_embedding and gbest_embedding. This is an expensive way to compute a simple weighted sum.

**Fix — Enrich the context:**
- Include **per-particle pbest embeddings** in the context.
- Include **multiple graph-level summaries** (different aggregations: mean, max, std of edge embeddings).
- Or simplify: replace cross-attention with a direct concatenation + MLP, which would be cheaper and equally expressive given 2 context tokens.

### 3.5 Sigma Ranges May Be Too Tight

**File:** `nets/tsp_vector.py`

```python
w_sigma = 0.05 + 0.1 * sigmoid(...)   # Range [0.05, 0.15]
c1_sigma = 0.1 + 0.2 * sigmoid(...)   # Range [0.1, 0.3]
c2_sigma = 0.1 + 0.2 * sigmoid(...)   # Range [0.1, 0.3]
```

With `c1_mu ∈ [1.0, 3.0]` and `c1_sigma ∈ [0.1, 0.3]`, the coefficient of variation is only ~5-15%. REINFORCE needs sufficient exploration to estimate gradients. If the sigma is too small, the network barely explores and the gradient estimate has high bias.

**Fix:** Start with larger sigma and let the network learn to reduce it:
```python
w_sigma = 0.1 + 0.3 * sigmoid(...)    # Range [0.1, 0.4]
c1_sigma = 0.2 + 0.8 * sigmoid(...)   # Range [0.2, 1.0]
c2_sigma = 0.2 + 0.8 * sigmoid(...)   # Range [0.2, 1.0]
```

Or add an **entropy bonus** to the loss to encourage exploration:
```python
entropy = wc1c2_dist.entropy().sum(dim=-1).mean()
loss = reinforce_loss - entropy_coef * entropy
```

---

## 4. Minor Issues & Optimizations

### 4.1 Batch Size = 1

Each training step processes a single TSP instance. This means:
- Very high gradient variance (different random instances each step).
- Slow convergence.

**Fix:** Process multiple instances in parallel. This requires batching the PSO population across instances, which is more complex but dramatically improves training stability. At minimum, accumulate gradients over multiple instances before stepping.

### 4.2 Learning Rate

`lr=1e-3` for Adam with REINFORCE can be too high, especially with the noisy gradients from single-instance training.

**Fix:** Try `lr=1e-4` or use a scheduler:
```python
def configure_optimizers(self):
    optimizer = torch.optim.Adam(self.net.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)
    return [optimizer], [scheduler]
```

### 4.3 No Gradient Clipping

The gradient clipping is commented out:
```python
# self.clip_gradients(opt, gradient_clip_val=1.5, gradient_clip_algorithm="norm")
```

REINFORCE gradients can have high variance and occasional spikes. Gradient clipping is important for stability.

**Fix:** Uncomment and use gradient clipping (e.g., max_norm=1.0).

### 4.4 `k_sparse = 20` for TSP-20 Means Fully Connected

With `n_cities=20` and `k_sparse=20`, every node connects to every other node. This means:
- The sparse graph structure provides no inductive bias.
- The particle dimension is `20 * 20 = 400`, which is unnecessarily large.
- For TSP-20, `k_sparse=5` or `10` would be sufficient (nearest neighbors).

**Fix:** Use a smaller `k_sparse` to reduce dimensionality and introduce sparsity bias. For TSP-20, try `k_sparse=10`.

### 4.5 `softmax_temperature=10.0` is Unused

The constructor receives `softmax_temperature` but it's never used in the forward pass. If it was intended for the decode step, it should be applied there.

---

## 5. Fundamental Design Considerations

### 5.1 Is PSO the Right Inner Optimizer for This Representation?

The particle representation (continuous edge weights decoded via argmax) has a fundamental mismatch with PSO:

- PSO works best when the **fitness landscape is continuous** and **smooth**. Small position changes should yield small cost changes.
- With argmax decoding, the fitness landscape is **piecewise constant** — a large region of position space maps to the same tour. PSO's velocity-based movement provides no useful gradient approximation.

**Alternatives to consider:**
1. **Stochastic decoding** (sampling from Categorical) makes the cost landscape stochastic but differentiable in expectation, which is much more PSO-friendly.
2. **Permutation-based PSO** (Clerc, 2004) where positions are permutations and velocity is a sequence of swaps. This is more natural for TSP but harder to integrate with neural networks.
3. **2-opt / local search as the inner optimizer** instead of PSO. The neural network could guide which 2-opt moves to make.

### 5.2 What Should the Neural Network Actually Learn?

Currently, the network predicts **scalar hyperparameters** (w, c1, c2) per particle. Even if the predictions are perfect, the PSO update rule is:

```
v = w*v + c1*r1*(pbest - x) + c2*r2*(gbest - x)
```

The directions (toward pbest and toward gbest) are **fixed by the PSO algorithm**. The network can only control the **magnitudes**. This is a very limited action space. The network cannot suggest novel search directions.

**More expressive alternatives:**
1. **Predict the full velocity update** directly: `v_new = net(x, v, pbest, gbest, graph)`. This gives the network complete control.
2. **Predict per-dimension hyperparameters**: Instead of 3 scalars per particle, predict `(w_i, c1_i, c2_i)` per dimension. Shape: `(n_particles, dim, 3)`.
3. **Predict additional attractor points**: Beyond pbest and gbest, the network could suggest additional targets for particles to move toward.

### 5.3 Consider the ACO Alternative

Your codebase already has an ACO (Ant Colony Optimization) module (`deep_aco_module.py`, `nets/aco.py`). ACO is arguably more natural for TSP because:
- It directly constructs solutions edge-by-edge.
- The pheromone matrix is analogous to your edge weight representation.
- The neural network can predict pheromone updates, which directly affect solution construction.
- There's a rich literature on neural-guided ACO (e.g., DeepACO, ICML 2023).

---

## 6. Recommended Action Plan (Priority Order)

| Priority | Change | Expected Impact | Effort |
|----------|--------|----------------|--------|
| **P0** | Diverse particle initialization (§2.1) | Fix zero-signal problem | Low |
| **P0** | Stochastic decoding during training (§2.2) | Enable meaningful RL signal | Low |
| **P1** | Velocity clamping (§2.4) | Prevent divergence | Low |
| **P1** | Accumulate loss, single update per step (§2.3) | Training stability | Medium |
| **P1** | Improvement-based reward (§3.1) | Better reward signal | Low |
| **P1** | Enable gradient clipping (§4.3) | Training stability | Trivial |
| **P2** | Larger sigma ranges + entropy bonus (§3.5) | Better exploration | Low |
| **P2** | Cache TSP embedding (§3.3) | Efficiency (~2x speedup) | Low |
| **P2** | Reduce `k_sparse` (§4.4) | Lower dimensionality | Trivial |
| **P2** | Reduce learning rate (§4.2) | Training stability | Trivial |
| **P3** | Richer context for cross-attention (§3.4) | Better predictions | Medium |
| **P3** | Edge-aware swarm encoder (§3.2) | Better particle encoding | Medium |
| **P3** | Per-dimension hyperparameters (§5.2) | More expressive control | High |

**Start with all P0 and P1 changes.** They are mostly independent, low-effort, and address the root causes. Then validate that the training signal improves before investing in architectural changes (P2, P3).
